{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['except', 'Except', 'thereby', 'Thereby', 'in', 'In', 'those', 'Those', 'whose', 'Whose', 'your', 'Your', 'take', 'Take', 'much', 'Much', 'five', 'Five', 'yours', 'Yours', 'co', 'Co', 'both', 'Both', 'since', 'Since', 'she', 'She', 'has', 'Has', 'so', 'So', 'four', 'Four', 'side', 'Side', 'call', 'Call', 'forty', 'Forty', 'through', 'Through', 'ie', 'Ie', 'the', 'The', 'nobody', 'Nobody', 'eg', 'Eg', 'are', 'Are', 'another', 'Another', 'cant', 'Cant', 'been', 'Been', 'rather', 'Rather', 'than', 'Than', 'serious', 'Serious', 'seeming', 'Seeming', 'something', 'Something', 'namely', 'Namely', 'interest', 'Interest', 'here', 'Here', 'all', 'All', 'above', 'Above', 'well', 'Well', 'be', 'Be', 'against', 'Against', 'yourself', 'Yourself', 'amongst', 'Amongst', 'by', 'By', 'ourselves', 'Ourselves', 'what', 'What', 'move', 'Move', 'us', 'Us', 'a', 'A', 'up', 'Up', 'former', 'Former', 'now', 'Now', 'anyhow', 'Anyhow', 'system', 'System', 'cry', 'Cry', 'but', 'But', 'am', 'Am', 'thick', 'Thick', 'give', 'Give', 'everything', 'Everything', 'last', 'Last', 'you', 'You', 'again', 'Again', 'elsewhere', 'Elsewhere', 'before', 'Before', 'still', 'Still', 're', 'Re', 'thin', 'Thin', 'if', 'If', 'them', 'Them', 'amoungst', 'Amoungst', 'together', 'Together', 'further', 'Further', 'etc', 'Etc', 'whatever', 'Whatever', 'therein', 'Therein', 'below', 'Below', 'three', 'Three', 'detail', 'Detail', 'fire', 'Fire', 'his', 'His', 'either', 'Either', 'name', 'Name', 'whenever', 'Whenever', 'toward', 'Toward', 'several', 'Several', 'full', 'Full', 'whole', 'Whole', 'show', 'Show', 'very', 'Very', 'please', 'Please', 'anywhere', 'Anywhere', 'find', 'Find', 'every', 'Every', 'because', 'Because', 'thereupon', 'Thereupon', 'amount', 'Amount', 'itself', 'Itself', 'alone', 'Alone', 'somehow', 'Somehow', 'perhaps', 'Perhaps', 'via', 'Via', 'less', 'Less', 'ten', 'Ten', 'between', 'Between', 'wherein', 'Wherein', 'get', 'Get', 'her', 'Her', 'everyone', 'Everyone', 'who', 'Who', 'whoever', 'Whoever', 'afterwards', 'Afterwards', 'for', 'For', 'enough', 'Enough', 'mostly', 'Mostly', 'otherwise', 'Otherwise', 'same', 'Same', 'can', 'Can', 'wherever', 'Wherever', 'whether', 'Whether', 'nine', 'Nine', 'done', 'Done', 'bill', 'Bill', 'behind', 'Behind', 'sometime', 'Sometime', 'cannot', 'Cannot', 'seem', 'Seem', 'this', 'This', 'hasnt', 'Hasnt', 'thru', 'Thru', 'almost', 'Almost', 'beyond', 'Beyond', 'around', 'Around', 'due', 'Due', 'ever', 'Ever', 'formerly', 'Formerly', 'hundred', 'Hundred', 'out', 'Out', 'someone', 'Someone', 'themselves', 'Themselves', 'nowhere', 'Nowhere', 'sixty', 'Sixty', 'noone', 'Noone', 'sincere', 'Sincere', 'nothing', 'Nothing', 'made', 'Made', 'none', 'None', 'may', 'May', 'see', 'See', 'two', 'Two', 'ours', 'Ours', 'my', 'My', 'hereby', 'Hereby', 'why', 'Why', 'then', 'Then', 'into', 'Into', 'throughout', 'Throughout', 'hers', 'Hers', 'whereafter', 'Whereafter', 'could', 'Could', 'as', 'As', 'front', 'Front', 'most', 'Most', 'within', 'Within', 'during', 'During', 'only', 'Only', 'among', 'Among', 'twelve', 'Twelve', 'i', 'I', 'therefore', 'Therefore', 'keep', 'Keep', 'might', 'Might', 'too', 'Too', 'him', 'Him', 'always', 'Always', 'whom', 'Whom', 'became', 'Became', 'it', 'It', 'third', 'Third', 'beforehand', 'Beforehand', 'how', 'How', 'over', 'Over', 'of', 'Of', 'one', 'One', 'found', 'Found', 'yet', 'Yet', 'with', 'With', 'whither', 'Whither', 'any', 'Any', 'being', 'Being', 'whereupon', 'Whereupon', 'own', 'Own', 'everywhere', 'Everywhere', 'me', 'Me', 'down', 'Down', 'back', 'Back', 'myself', 'Myself', 'beside', 'Beside', 'nevertheless', 'Nevertheless', 'often', 'Often', 'per', 'Per', 'anything', 'Anything', 'where', 'Where', 'without', 'Without', 'sometimes', 'Sometimes', 'eleven', 'Eleven', 'while', 'While', 'whereby', 'Whereby', 'hereafter', 'Hereafter', 'else', 'Else', 'part', 'Part', 'least', 'Least', 'must', 'Must', 'top', 'Top', 'seemed', 'Seemed', 'thence', 'Thence', 'mine', 'Mine', 'nor', 'Nor', 'had', 'Had', 'inc', 'Inc', 'no', 'No', 'until', 'Until', 'across', 'Across', 'latter', 'Latter', 'an', 'An', 'there', 'There', 'fifteen', 'Fifteen', 'hereupon', 'Hereupon', 'latterly', 'Latterly', 'from', 'From', 'few', 'Few', 'meanwhile', 'Meanwhile', 'onto', 'Onto', 'describe', 'Describe', 'their', 'Their', 'he', 'He', 'however', 'However', 'that', 'That', 'bottom', 'Bottom', 'other', 'Other', 'first', 'First', 'each', 'Each', 'neither', 'Neither', 'become', 'Become', 'though', 'Though', 'were', 'Were', 'such', 'Such', 'have', 'Have', 'con', 'Con', 'is', 'Is', 'will', 'Will', 'six', 'Six', 'un', 'Un', 'also', 'Also', 'although', 'Although', 'whereas', 'Whereas', 'and', 'And', 'hence', 'Hence', 'off', 'Off', 'should', 'Should', 'after', 'After', 'couldnt', 'Couldnt', 'next', 'Next', 'becomes', 'Becomes', 'when', 'When', 'de', 'De', 'twenty', 'Twenty', 'would', 'Would', 'they', 'They', 'whence', 'Whence', 'herself', 'Herself', 'others', 'Others', 'besides', 'Besides', 'towards', 'Towards', 'becoming', 'Becoming', 'more', 'More', 'empty', 'Empty', 'or', 'Or', 'was', 'Was', 'thus', 'Thus', 'these', 'These', 'fifty', 'Fifty', 'eight', 'Eight', 'mill', 'Mill', 'fill', 'Fill', 'once', 'Once', 'about', 'About', 'himself', 'Himself', 'under', 'Under', 'not', 'Not', 'on', 'On', 'its', 'Its', 'which', 'Which', 'many', 'Many', 'our', 'Our', 'thereafter', 'Thereafter', 'already', 'Already', 'anyone', 'Anyone', 'we', 'We', 'moreover', 'Moreover', 'ltd', 'Ltd', 'somewhere', 'Somewhere', 'do', 'Do', 'anyway', 'Anyway', 'put', 'Put', 'seems', 'Seems', 'yourselves', 'Yourselves', 'herein', 'Herein', 'some', 'Some', 'go', 'Go', 'even', 'Even', 'to', 'To', 'upon', 'Upon', 'along', 'Along', 'at', 'At', 'indeed', 'Indeed', 'never', 'Never']\n"
     ]
    }
   ],
   "source": [
    "from nltk import wsd, pos_tag\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "from pprint import pprint\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stopwords\n",
    "\n",
    "#you'll probably need to download some nltk packages\n",
    "#it should tell you what they are when you run this cell\n",
    "\n",
    "ic = wn.ic(brown) \n",
    "sw = []\n",
    "for w in stopwords:\n",
    "    \n",
    "    sw.append(w)\n",
    "    upper = w[0].upper() + w[1:]\n",
    "    sw.append(upper)\n",
    "print(sw)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the nltk pos tagger tags differently than wordnet, so I need this helper function\n",
    "def get_pos(tagged_word):\n",
    "    \n",
    "\n",
    "    \n",
    "    if tagged_word[1][0] == 'N':\n",
    "        pos = 'n'\n",
    "    elif tagged_word[1][0] == 'V':\n",
    "        pos = 'v'\n",
    "    elif tagged_word[1][0] == 'J':\n",
    "        pos = 'a'\n",
    "    elif tagged_word[1][0] == 'R':\n",
    "        pos = 'r'\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#helper to remove stopwords\n",
    "def filter(tokens):\n",
    "\n",
    "    filtered = [w for w in tokens if not w in sw and w.isalpha()]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sent_hypernyms(input_sent):\n",
    "    input_sent_tokens = filter(wt(input_sent))\n",
    "    sent_hypernyms = {}\n",
    "    for word in input_sent_tokens:\n",
    "        if word not in sent_hypernyms.keys():\n",
    "            sent_hypernyms[word] = get_broader_senses(word)\n",
    "    \n",
    "    return sent_hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_broader_senses(word):\n",
    "    senses = []\n",
    "    for ss in wn.synsets(word):\n",
    "        path = []\n",
    "        result = []\n",
    "        seen = set()\n",
    "        todo = [ss]\n",
    "        while todo:\n",
    "            next_synset = todo.pop()\n",
    "            path.append(next_synset)\n",
    "            if next_synset not in seen:\n",
    "                seen.add(next_synset)\n",
    "                next_hypernyms = (\n",
    "                    next_synset.hypernyms() + next_synset.instance_hypernyms()\n",
    "                )\n",
    "                if not next_hypernyms:\n",
    "                    result.append(next_synset)\n",
    "                else:\n",
    "                    todo.extend(next_hypernyms)\n",
    "        try:\n",
    "            #print(ss, ss.definition(), path[3], path[3].definition())\n",
    "            if path[3] not in senses:\n",
    "                senses.append(path[3])\n",
    "        except IndexError:\n",
    "            #print(ss, ss.definition(), path[-1], path[-1].definition())\n",
    "            if path[-1] not in senses:\n",
    "                senses.append(path[-1])\n",
    "        \n",
    "    return(senses)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provides dictionary with keys of nonstopwords from the input\n",
    "#which map to their basic wsd lesk-algorithm-provided senses\n",
    "#also provides lesk algorithm's assessment of our amibguous concept, which rarely goes well for ambiguous concepts\n",
    "def parse_input(my_word, input_sent):\n",
    "    my_pos = ''\n",
    "    input_sent = wt(input_sent)\n",
    "    filtered_input = filter(input_sent)\n",
    "    pos_input = pos_tag(input_sent)\n",
    "    naive_lesk_result = ()\n",
    "    input_senses = []\n",
    "    for word in pos_input:\n",
    "\n",
    "        if word[0] != my_word and word[0] in filtered_input:\n",
    "            try:\n",
    "                #lesk_result = wsd.lesk(input_sent, word[0], get_pos(word))\n",
    "                #input_lesk_senses[word[0]] = (lesk_result, lesk_result.definition())\n",
    "                senses = [ss for ss in wn.synsets(word[0], get_pos(word))]\n",
    "                for sense in senses:\n",
    "                    input_senses.append(sense)\n",
    "            except:\n",
    "                #lesk_result = wsd.lesk(input_sent, word[0])\n",
    "                #input_lesk_senses[word[0]] = (lesk_result, lesk_result.definition())\n",
    "                senses = [ss for ss in wn.synsets(word[0])]\n",
    "                for sense in senses:\n",
    "                    input_senses.append(sense)\n",
    "        elif word[0] == my_word:\n",
    "            my_pos = get_pos(word)\n",
    "            lesk_result = wsd.lesk(input_sent, word[0])\n",
    "            naive_lesk_result = (lesk_result, lesk_result.definition())\n",
    "            \n",
    "            \n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return input_senses, naive_lesk_result\n",
    "\n",
    "# parsed_input, my_pos, naive_lesk_result = parse_input(my_word, input_sent)\n",
    "# print(parsed_input, my_pos)\n",
    "# print('Naive lesk for ' + my_word + ': ', naive_lesk_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper(word):\n",
    "    pattern = '[a-z-]+'\n",
    "    hyper = {}\n",
    "    for ss in wn.synsets(word):\n",
    "        try:\n",
    "            hyper[ss.hypernyms()[0]] = ss.hypernyms()[0].definition()\n",
    "        except IndexError:\n",
    "            if ss.pos() == 's':\n",
    "                lemma = ss.lemmas()[0]\n",
    "                cs_head = re.findall(pattern, lemma.key())[-1]\n",
    "                head_sets = wn.synsets(cs_head)[0]\n",
    "                if head_sets.hypernyms():\n",
    "                    hyper[head_sets.hypernyms()[0]] = head_sets.hypernyms()[0].definition()\n",
    "                else:\n",
    "                    hyper[head_sets] = head_sets.definition()\n",
    "\n",
    "\n",
    "    \n",
    "    return hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(my_word, input_sent, verbose=False): \n",
    "    \n",
    "    parsed_input, naive_lesk_result = parse_input(my_word, input_sent)\n",
    "    pattern = '[a-z-]+'\n",
    "    count = 0\n",
    "    sent_hypernyms = make_sent_hypernyms(input_sent)\n",
    "    #print(sent_hypernyms)\n",
    "    #best overall matches for each score that isn't matched to itself\n",
    "    overall_best_pathsim_value = float('-inf')\n",
    "    overall_best_pathsim_pair = ()\n",
    "    overall_best_wup_value = float('-inf')\n",
    "    overall_best_wup_pair = ()\n",
    "    overall_best_lch_value = float('-inf')\n",
    "    overall_best_lch_pair = ()\n",
    "    overall_best_res_value = float('-inf')\n",
    "    overall_best_res_pair = ()\n",
    "    overall_best_jcn_value = float('-inf')\n",
    "    overall_best_jcn_pair = ()\n",
    "    overall_best_lin_value = float('-inf')\n",
    "    overall_best_lin_pair = ()\n",
    "    overall_best_lesk_value = 0\n",
    "    overall_best_lesk_pair = ()\n",
    "\n",
    "    #using all provided wordnet taxonomy methods\n",
    "    #calculate similarity between hypernym sense of each input_sentence word sense\n",
    "    #and hypernym sense of each word in the definitions of each concept sense's hypernym\n",
    "\n",
    "    #synsets = wn.synsets(my_word) #list of senses for ambigous concept, without matching part of speech\n",
    "\n",
    "    concept_data = dict.fromkeys(get_hyper(my_word).keys(), {})          \n",
    "                                #will contain all similarity values between concept-sense-definition-word-senses\n",
    "                                #and input sentence lesk-provided senses - d0\n",
    "                                #who said quadruple-nested dictionaries were bad? I know I didn't!\n",
    "    \n",
    "    concept_sense_scores = dict.fromkeys(concept_data.keys(), {}) #maps concept sense hypernyms to its overall_best scores for each metric\n",
    "    \n",
    "    #print(concept_data, concept_sense_scores)\n",
    "    #for each potential ambiguous concept hypernym sense - loop 0 (PCSH)\n",
    "    for syn in concept_data:\n",
    "        \n",
    "        \n",
    "        best_syn_scores = {'pathsim' : ((), float('-inf')),\n",
    "                           'wup' : ((), float('-inf')),\n",
    "                           'lch' : ((), float('-inf')),\n",
    "                           'res' : ((), float('-inf')),\n",
    "                           'jcn' : ((), float('-inf')),\n",
    "                           'lin' : ((), float('-inf')),\n",
    "                           'lesk' : ((), 0)\n",
    "                          }\n",
    "        \n",
    "        definition = wt(syn.definition())\n",
    "        def_pos = pos_tag(definition) #get pos_tags for current sense definition\n",
    "        def_filtered = filter(definition) #remove stop words\n",
    "        def_synsets = []\n",
    "        \n",
    "        syn_def = {}    #dictionary of definition-word-senses for current ambiguous concept sense\n",
    "                        #maps to dictionary of definition-word-senses for each word of current sense's definition - d1\n",
    "        \n",
    "        #print(def_pos)\n",
    "        #removes stopwords from PCSH definition and gets definition word senses\n",
    "        for def_word in def_pos:\n",
    "            if def_word[0] in def_filtered:\n",
    "                #create list of current definition-word's senses\n",
    "                current_pos = get_pos(def_word)\n",
    "                def_synsets = wn.synsets(def_word[0], current_pos)\n",
    "                \n",
    "                if not def_synsets:\n",
    "                    def_synsets = wn.synsets(def_word[0])\n",
    "            \n",
    "            hyp_synsets = []\n",
    "            #print(def_synsets)   \n",
    "            for ss in def_synsets:\n",
    "                if ss.hypernyms() and ss.hypernyms()[0] not in hyp_synsets:\n",
    "                    hyp_synsets.append(ss.hypernyms()[0])\n",
    "                    inner = {ss.hypernyms()[0] : {}}\n",
    "                    syn_def[ss] = inner\n",
    "\n",
    "\n",
    "        #print(syn_def)\n",
    "                        \n",
    "     \n",
    "        #for each nonstopword hypernym in the defintion of the current ambiguous concept sense\n",
    "        #matches for part of speech - loop 1 (PCDSWH)\n",
    "        for item in syn_def:\n",
    "            syn_def_syn = sent_hypernyms\n",
    "                            #dictionary of current concept-sense's definition's current word's hypernym\n",
    "                            #mapped to dict of each input sentence word-sense's similarity score - d2\n",
    "            \n",
    "            item_head = list(syn_def[item].keys())[0]\n",
    "            #print(item, item_head, syn_def_syn, '\\n')\n",
    "\n",
    "#             changed_item = False\n",
    "#             item_head = ()\n",
    "#             #handle wordnet's annoying satellite adjective tag\n",
    "#             if item.pos() == 's':\n",
    "\n",
    "#                 lemma = item.lemmas()[0]\n",
    "#                 head = re.findall(pattern, lemma.key())[-1]\n",
    "#                 head_sets = wn.synsets(head)\n",
    "\n",
    "#                 #todo: figure out better way to handle multiple head adjective option\n",
    "\n",
    "#                 item_head = [s for s in head_sets if s.pos() == 'a'][0]\n",
    "\n",
    "\n",
    "\n",
    "            #calculate similarity between current definition-word sense and each input_sentence lesk-sense\n",
    "            for input_word in syn_def_syn:\n",
    "                best_current_input_similarity = {'pathsim_value' : None, #similarity dict for current input_sentence word - d3\n",
    "                                                 'wup_value' : None,\n",
    "                                                 'lch_value': None,\n",
    "                                                 'res_value' : None,\n",
    "                                                 'jcn_value' : None,\n",
    "                                                 'lin_value' : None,\n",
    "                                                 'lesk_value' : None \n",
    "                                                }\n",
    "                for current_sense in syn_def_syn[input_word]:\n",
    "                    same = False\n",
    "                    similarity_scores = {'pathsim_value' : None, #similarity dict for current input_sentence word sense\n",
    "                                         'wup_value' : None,\n",
    "                                         'lch_value': None,\n",
    "                                         'res_value' : None,\n",
    "                                         'jcn_value' : None,\n",
    "                                         'lin_value' : None,\n",
    "                                         'lesk_value' : None \n",
    "                                        }\n",
    "\n",
    "                    print(item_head, current_sense, syn_def_syn[input_word])\n",
    "                    #current_sense = syn_def_syn[sense].key()\n",
    "\n",
    "                    #lesk algorithm, modified from https://www.nltk.org/_modules/nltk/wsd.html\n",
    "                    context = set(filter(wt(current_sense.definition())))\n",
    "\n",
    "                    overlap = context.intersection(filter(wt(item_head.definition())))\n",
    "                    similarity_scores['lesk_value'] = len(overlap)\n",
    "\n",
    "                    if len(overlap) > best_syn_scores['lesk'][1]:\n",
    "                        best_syn_scores['lesk'] = ((item_head, current_sense), len(overlap))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    if current_sense.pos() == 's' and item_head.pos() == 'a':\n",
    "\n",
    "                        lemma = current_sense.lemmas()[0]\n",
    "                        cs_head = re.findall(pattern, lemma.key())[-1]\n",
    "                        head_sets = wn.synsets(cs_head)\n",
    "                        try:\n",
    "                            current_sense = [s for s in head_sets if s.pos() == 'a'][0]\n",
    "\n",
    "                            #print(current_sense, current_sense.definition(), definition)\n",
    "                            if current_sense == item_head:\n",
    "                                print('same adj: ', definition(), current_sense, current_sense.definition())\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    elif current_sense.pos() != item_head.pos():\n",
    "                        continue\n",
    "\n",
    "\n",
    "\n",
    "                    if current_sense == item_head:\n",
    "                        same = True\n",
    "\n",
    "                    if len(overlap) > overall_best_lesk_value and not same:\n",
    "                        overall_best_lesk_value = len(overlap)\n",
    "                        overall_best_lesk_pair = (item_head, current_sense)\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        #https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset.path_similarity\n",
    "                        pathsim_value = item_head.path_similarity(current_sense)\n",
    "                        similarity_scores['pathsim_value'] = pathsim_value\n",
    "                        if pathsim_value and pathsim_value > best_syn_scores['pathsim'][1]:\n",
    "                            best_syn_scores['pathsim'] = ((item_head, current_sense), pathsim_value)\n",
    "\n",
    "                        if pathsim_value and pathsim_value > overall_best_pathsim_value and not same:\n",
    "                            overall_best_pathsim_value = pathsim_value\n",
    "                            overall_best_pathsim_pair = (item_head, current_sense)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        if type(e) != TypeError:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        #https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset.wup_similarity\n",
    "                        wup_value = item_head.wup_similarity(current_sense)\n",
    "                        similarity_scores['wup_value'] = wup_value\n",
    "                        if wup_value and wup_value > best_syn_scores['wup'][1]:\n",
    "                            best_syn_scores['wup'] = ((item_head, current_sense), wup_value)\n",
    "\n",
    "                        if wup_value and wup_value > overall_best_wup_value and not same:\n",
    "                            overall_best_wup_value = wup_value\n",
    "                            overall_best_wup_pair = (item_head, current_sense)\n",
    "                            if wup_value >= 0.8:\n",
    "                                print(item_head, current_sense, ' wup: ', wup_value)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(item, sense, ' failed wup', e)\n",
    "\n",
    "                    try:\n",
    "                        #https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset.lch_similarity\n",
    "                        lch_value = item_head.lch_similarity(current_sense)\n",
    "                        similarity_scores['lch_value'] = lch_value\n",
    "                        if lch_value and lch_value > best_syn_scores['lch'][1]:\n",
    "                            best_syn_scores['lch'] = ((item_head, current_sense), lch_value)\n",
    "\n",
    "                        if lch_value and lch_value > overall_best_lch_value and not same:\n",
    "                            overall_best_lch_value = lch_value\n",
    "                            overall_best_lch_pair = (item_head, current_sense)\n",
    "                            if lch_value >= 1.5:\n",
    "                                print(item_head, current_sense, ' lch: ', lch_value)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(item_head, current_sense, ' failed  lch', e)\n",
    "\n",
    "                    try:\n",
    "                        #https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset.res_similarity\n",
    "                        res_value = item_head.res_similarity(current_sense, ic)\n",
    "                        similarity_scores['res_value'] = res_value\n",
    "                        if res_value and res_value > best_syn_scores['res'][1]:\n",
    "                            best_syn_scores['res'] = ((item_head, current_sense), res_value)\n",
    "\n",
    "                        if res_value and res_value > overall_best_res_value and not same:\n",
    "                            overall_best_res_value = res_value\n",
    "                            overall_best_res_pair = (item_head, current_sense)\n",
    "                            if res_value >= 2.5:  \n",
    "                                print(item_head, current_sense, ' res: ', res_value)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(item_head, current_sense, ' failed res', e)\n",
    "\n",
    "                    try:\n",
    "                        ##https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset.jcn_similarity\n",
    "                        jcn_value = item_head.jcn_similarity(current_sense, ic)\n",
    "                        similarity_scores['jcn_value'] = jcn_value\n",
    "                        if jcn_value and jcn_value > best_syn_scores['jcn'][1]:\n",
    "                            best_syn_scores['jcn'] = ((item_head, current_sense), jcn_value)\n",
    "\n",
    "                        if jcn_value and jcn_value > overall_best_jcn_value and not same:\n",
    "                            overall_best_jcn_value = jcn_value\n",
    "                            overall_best_jcn_pair = (item_head, current_sense)\n",
    "                            if jcn_value >= 0.1:\n",
    "                                print(item_head, current_sense, ' jcn: ', jcn_value)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(item_head, current_sense, ' failed jcn', e)\n",
    "\n",
    "                    try:\n",
    "                        #https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordnet.Synset.lin_similarity\n",
    "                        lin_value = item_head.lin_similarity(current_sense, ic)\n",
    "                        similarity_scores['lin_value'] = lin_value\n",
    "                        if lin_value and lin_value > best_syn_scores['lin'][1]:\n",
    "                            best_syn_scores['lin'] = ((item_head, current_sense), lin_value)\n",
    "\n",
    "                        if lin_value and lin_value > overall_best_lin_value and not same:\n",
    "                            overall_best_lin_value = lin_value\n",
    "                            overall_best_lin_pair = (item_head, current_sense)\n",
    "                            if lin_value >= 0.25:\n",
    "                                print(item_head, current_sense, ' lin: ', lin_value)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(item_head, current_sense, ' failed lin', e)\n",
    "\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            syn_dict[item].update(value_dict)\n",
    "\n",
    "        concept_data[syn].update(syn_dict)\n",
    "\n",
    "    concept_sense_scores[syn].update(best_syn_scores)\n",
    "\n",
    "\n",
    "    print('overall_best_pathsim: ', overall_best_pathsim_pair, overall_best_pathsim_value)\n",
    "    print('overall_best_wup: ', overall_best_wup_pair, overall_best_wup_value)\n",
    "    print('overall_best_lch: ', overall_best_lch_pair, overall_best_lch_value)\n",
    "    print('overall_best_res: ', overall_best_res_pair, overall_best_res_value)\n",
    "    print('overall_best_jcn: ', overall_best_jcn_pair, overall_best_jcn_value)\n",
    "    print('overall_best_lin: ', overall_best_lin_pair, overall_best_lin_value)\n",
    "    print('overall_best_lesk: ', overall_best_lesk_pair, overall_best_lesk_value)\n",
    "    print('naive_lesk', naive_lesk_result)\n",
    "\n",
    "    print(count)\n",
    "    \n",
    "    if verbose:\n",
    "        print(concept)\n",
    "        \n",
    "    #return concept, parsed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('region.n.03') Synset('condition.n.01') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('condition.n.01')  jcn:  0.13960197487606174\n",
      "Synset('region.n.03') Synset('state.n.02') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('state.n.02')  lch:  1.55814461804655\n",
      "Synset('region.n.03') Synset('state.n.02')  jcn:  0.16412322724351902\n",
      "Synset('region.n.03') Synset('physical_entity.n.01') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('physical_entity.n.01')  lch:  2.2512917986064953\n",
      "Synset('region.n.03') Synset('physical_entity.n.01')  jcn:  0.48771153119098515\n",
      "Synset('region.n.03') Synset('physical_entity.n.01')  lin:  0.43359586891753665\n",
      "Synset('region.n.03') Synset('desire.v.01') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('be.v.01') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('search.v.01') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('communicate.v.01') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('miss.v.06') [Synset('condition.n.01'), Synset('state.n.02'), Synset('physical_entity.n.01'), Synset('desire.v.01'), Synset('be.v.01'), Synset('search.v.01'), Synset('communicate.v.01'), Synset('miss.v.06')]\n",
      "Synset('region.n.03') Synset('artifact.n.01') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('artifact.n.01')  lin:  0.44731860761787756\n",
      "Synset('region.n.03') Synset('whole.n.02') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('whole.n.02')  jcn:  0.5111116830138273\n",
      "Synset('region.n.03') Synset('whole.n.02')  lin:  0.5474343999239438\n",
      "Synset('region.n.03') Synset('entertainer.n.01') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('plan_of_action.n.01') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('move.v.02') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('connect.v.01') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('make.v.03') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('give.v.03') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('transfer.v.02') [Synset('artifact.n.01'), Synset('whole.n.02'), Synset('entertainer.n.01'), Synset('plan_of_action.n.01'), Synset('move.v.02'), Synset('connect.v.01'), Synset('make.v.03'), Synset('give.v.03'), Synset('transfer.v.02')]\n",
      "Synset('region.n.03') Synset('vascular_plant.n.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('drug_of_abuse.n.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('color.n.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('travel.v.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('change_magnitude.v.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('change.v.02') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('be.v.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('become.v.03') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('change.v.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('get_up.v.02') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('rise.v.12') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('set_about.v.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('refute.v.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('rose.s.01') [Synset('vascular_plant.n.01'), Synset('drug_of_abuse.n.01'), Synset('color.n.01'), Synset('travel.v.01'), Synset('change_magnitude.v.01'), Synset('change.v.02'), Synset('be.v.01'), Synset('become.v.03'), Synset('change.v.01'), Synset('get_up.v.02'), Synset('rise.v.12'), Synset('set_about.v.01'), Synset('refute.v.01'), Synset('rose.s.01')]\n",
      "Synset('region.n.03') Synset('new.a.01') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('fresh.s.04') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('raw.s.12') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('new.s.04') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('new.s.05') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('new.a.06') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('newfangled.s.01') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('new.s.08') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('modern.s.05') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('new.s.10') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('new.s.11') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('newly.r.01') [Synset('new.a.01'), Synset('fresh.s.04'), Synset('raw.s.12'), Synset('new.s.04'), Synset('new.s.05'), Synset('new.a.06'), Synset('newfangled.s.01'), Synset('new.s.08'), Synset('modern.s.05'), Synset('new.s.10'), Synset('new.s.11'), Synset('newly.r.01')]\n",
      "Synset('region.n.03') Synset('geographical_area.n.01') [Synset('geographical_area.n.01'), Synset('group.n.01'), Synset('tract.n.01'), Synset('care.v.02')]\n",
      "Synset('region.n.03') Synset('geographical_area.n.01')  wup:  0.9090909090909091\n",
      "Synset('region.n.03') Synset('geographical_area.n.01')  lch:  2.9444389791664407\n",
      "Synset('region.n.03') Synset('geographical_area.n.01')  res:  2.8352044900650055\n",
      "Synset('region.n.03') Synset('geographical_area.n.01')  jcn:  0.6153231434617166\n",
      "Synset('region.n.03') Synset('geographical_area.n.01')  lin:  0.7772398803210264\n",
      "Synset('region.n.03') Synset('group.n.01') [Synset('geographical_area.n.01'), Synset('group.n.01'), Synset('tract.n.01'), Synset('care.v.02')]\n",
      "Synset('region.n.03') Synset('tract.n.01') [Synset('geographical_area.n.01'), Synset('group.n.01'), Synset('tract.n.01'), Synset('care.v.02')]\n",
      "Synset('region.n.03') Synset('care.v.02') [Synset('geographical_area.n.01'), Synset('group.n.01'), Synset('tract.n.01'), Synset('care.v.02')]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'syn_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-f77cc8527f8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minput_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'I want to plant this rose in my new garden'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(parse_input(my_word, input_sent))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(parsed_input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#input_sent_hypers = make_sent_hypernyms(input_sent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-58c4604e4ca7>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(my_word, input_sent, verbose)\u001b[0m\n\u001b[0;32m    263\u001b[0m                     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m             \u001b[0msyn_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mconcept_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msyn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyn_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'syn_dict' is not defined"
     ]
    }
   ],
   "source": [
    "my_word = 'garden'\n",
    "input_sent = 'I want to plant this rose in my new garden'\n",
    "#print(parse_input(my_word, input_sent))\n",
    "main(my_word, input_sent, verbose=False)\n",
    "#print(parsed_input)\n",
    "#input_sent_hypers = make_sent_hypernyms(input_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
